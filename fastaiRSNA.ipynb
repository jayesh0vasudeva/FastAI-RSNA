{"cells":[{"metadata":{},"cell_type":"markdown","source":"# fastai starter\n\nMany thanks to [Basic EDA + Data Visualization ðŸ§  ](https://www.kaggle.com/marcovasquez/basic-eda-data-visualization) for the code to load the data."},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.models import *\n!pip install pretrainedmodels\nimport pretrainedmodels as pm\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\nimport fastai\nfrom utils import *\nimport sys\nimport torch\nfastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import glob, pylab, pandas as pd\nimport pydicom, numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import ndimage\nimport scipy.misc\nfrom skimage import morphology\nfrom skimage.segmentation import slic\nfrom skimage import measure\nfrom skimage.transform import resize, warp\nfrom skimage import exposure\n# Some machine learning as a treat\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and preprocess data\n\nWe will transform the data into a nice space separated label format."},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy pretrained weights to the folder fastai will search by default\nPath('/tmp/.cache/torch/checkpoints/').mkdir(exist_ok=True, parents=True)\nmodel_path = '/tmp/.cache/torch/checkpoints/efficientNet.pth'\n!cp ../input/efficientnet*/efficientNet_*.pth {model_path}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"package_path = '../input/efficientnet-pytorch/efficientnet-pytorch/EfficientNet-PyTorch-master'\nsys.path.append(package_path)\n\nfrom efficientnet_pytorch import EfficientNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FastAI adapators to retrain our model without lossing its old head ;)\ndef EfficientNetB0(pretrained=True):\n    \"\"\"Constructs a EfficientNet model for FastAI.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = EfficientNet.from_name('efficientnet-b3', override_params={'num_classes':6})\n\n    if pretrained:\n        model_state = torch.load(\"../input/efficientnet-pytorch/efficientnet-b3-c8376fa2.pth\")\n        # load original weights apart from its head\n        if '_fc.weight' in model_state.keys():\n            model_state.pop('_fc.weight')\n            model_state.pop('_fc.bias')\n            res = model.load_state_dict(model_state, strict=False)\n            assert str(res.missing_keys) == str(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n        else:\n            # A basic remapping is required\n            from collections import OrderedDict\n            mapping = { i:o for i,o in zip(model_state.keys(), model.state_dict().keys()) }\n            mapped_model_state = OrderedDict([\n                (mapping[k], v) for k,v in model_state.items() if not mapping[k].startswith('_fc')\n            ])\n            res = model.load_state_dict(mapped_model_state, strict=False)\n            print(res)\n    return model\n# create model\nmodel = EfficientNetB0(pretrained=True)\n# print model structure (hidden)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA = Path(\"../input/rsna-intracranial-hemorrhage-detection\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/stage_1_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape) # shape of original dataset\ndf=df.drop(df.index[df[df.ID.str.startswith('ID_6431af929')].index.values])\nprint(df.shape) # shape after deleting the corrupt\ndf.index=np.arange(len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newtable=df.copy()\nnew = newtable[\"ID\"].str.split(\"_\", n = 1, expand = True)\nnewX = new[1].str.split(\"_\", n = 1, expand = True)\nnewX[1]\nnewtable['Image_ID'] = newX[0]\nnewtable['Sub_type'] = newX[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newtable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_ids = newtable.Image_ID.unique()\nlabels = [\"\" for _ in range(len(image_ids))]\nnew_df = pd.DataFrame(np.array([image_ids, labels]).transpose(), columns=[\"id\", \"labels\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lbls = {i : \"\" for i in image_ids}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new = newtable[newtable.Label == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new.Sub_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newtable = newtable[newtable.Label == 1]\n#newtable = newtable[newtable.Sub_type != \"any\"]\n\ni = 0\nfor name, group in newtable.groupby(\"Image_ID\"):\n    lbls[name] = \" \".join(group.Sub_type)\n    if i % 10000 == 0: print(i)\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = pd.DataFrame(np.array([list(lbls.keys()), list(lbls.values())]).transpose(), columns=[\"id\", \"labels\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del lbls\ndel newtable\ndel newX\ndel new\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# fastai Dataset\n\nThanks to this kernel for the code to apply the windowing: [EDA: View dicom images with correct windowing](https://www.kaggle.com/omission/eda-view-dicom-images-with-correct-windowing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/omission/eda-view-dicom-images-with-correct-windowing\n\ndef window_image(img, window_center,window_width, intercept, slope):\n\n    img = (img*slope +intercept)\n    img_min = window_center - window_width/2\n    img_max = window_center + window_width/2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    return img\n\ndef get_first_of_dicom_field_as_int(x):\n    #get x[0] as in int is x is a 'pydicom.multival.MultiValue', otherwise get int(x)\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef get_windowing(data):\n    dicom_fields = [data[('0028','1050')].value, #window center\n                    data[('0028','1051')].value, #window width\n                    data[('0028','1052')].value, #intercept\n                    data[('0028','1053')].value] #slope\n    return [get_first_of_dicom_field_as_int(x) for x in dicom_fields]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_resample(image, dicom_header, new_spacing=[1,1]):\n    # Code from https://www.raddq.com/dicom-processing-segmentation-visualization-in-python/\n    # Adapted to work for pixels.\n    spacing = map(float, dicom_header.PixelSpacing)\n    spacing = np.array(list(spacing))\n    resize_factor = spacing / new_spacing\n    new_real_shape = image.shape * resize_factor\n    new_shape = np.round(new_real_shape)\n    real_resize_factor = new_shape / image.shape\n    new_spacing = spacing / real_resize_factor\n    \n    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n    \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.id = \"ID_\" + new_df.id + \".dcm\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_crop(image):\n    # Based on this stack overflow post: https://stackoverflow.com/questions/26310873/how-do-i-crop-an-image-on-a-white-background-with-python\n    mask = image == 0\n\n    # Find the bounding box of those pixels\n    coords = np.array(np.nonzero(~mask))\n    top_left = np.min(coords, axis=1)\n    bottom_right = np.max(coords, axis=1)\n\n    out = image[top_left[0]:bottom_right[0],\n                top_left[1]:bottom_right[1]]\n    \n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_window(img, window_center, window_width, U=1.0, eps=(1.0 / 255.0)):\n    _, _, intercept, slope = get_windowing(img)\n    img = img.pixel_array * slope + intercept\n    ue = np.log((U / eps) - 1.0)\n    W = (2 / window_width) * ue\n    b = ((-2 * window_center) / window_width) * ue\n    z = W * img + b\n    img = U / (1 + np.power(np.e, -1.0 * z))\n    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n    return img\n\ndef map_to_gradient_sig(grey_img):\n    rainbow_img = np.zeros((grey_img.shape[0], grey_img.shape[1], 3))\n    rainbow_img[:, :, 0] = np.clip(4*grey_img - 2, 0, 1.0) * (grey_img > 0.01) * (grey_img <= 1.0)\n    rainbow_img[:, :, 1] =  np.clip(4*grey_img * (grey_img <=0.75), 0,1) + np.clip((-4*grey_img + 4) * (grey_img > 0.75), 0, 1)\n    rainbow_img[:, :, 2] = np.clip(-4*grey_img + 2, 0, 1.0) * (grey_img > 0.01) * (grey_img <= 1.0)\n    return rainbow_img\n\ndef sigmoid_rainbow_bsb_window(img):\n    brain_img = sigmoid_window(img, 40, 80)\n    subdural_img = sigmoid_window(img, 80, 200)\n    bone_img = sigmoid_window(img, 600, 2000)\n    combo = (brain_img*0.35 + subdural_img*0.5 + bone_img*0.15)\n    combo_norm = (combo - np.min(combo)) / (np.max(combo) - np.min(combo))\n    return map_to_gradient_sig(combo_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_open_image(path, div=True, convert_mode=None, after_open=None):\n    dcm = pydicom.dcmread(str(path))\n    im=sigmoid_rainbow_bsb_window(dcm)\n    #window_center, window_width, intercept, slope = get_windowing(dcm)\n    #im = window_image(dcm.pixel_array,window_center, window_width, intercept, slope)\n    #img = image_resample(img,dcm,new_spacing=[1,1])\n    #im = np.stack((img,)*3, axis=-1)\n    im -= im.min()\n    im_max = im.max()\n    if im_max != 0: im = im / im.max()\n    x = Image(pil2tensor(im, dtype=np.float32))\n    #if div: x.div_(2048)  # ??\n    return x\nvision.data.open_image = new_open_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.concat([new_df[new_df.labels == \"\"][:], new_df[new_df.labels != \"\"][:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def se_resnext50_32x4d(pretrained=True):\n    pretrained = 'imagenet' if pretrained else None\n    model = pm.xception(pretrained=pretrained)\n    return nn.Sequential(*list(model.children()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AccumulateOptimWrapper(OptimWrapper):\n    def step(self):           pass\n    def zero_grad(self):      pass\n    def real_step(self):      super().step()\n    def real_zero_grad(self): super().zero_grad()\n        \ndef acc_create_opt(self, lr:Floats, wd:Floats=0.):\n        \"Create optimizer with `lr` learning rate and `wd` weight decay.\"\n        self.opt = AccumulateOptimWrapper.create(self.opt_func, lr, self.layer_groups,\n                                         wd=wd, true_wd=self.true_wd, bn_wd=self.bn_wd)\nLearner.create_opt = acc_create_opt   \n\n@dataclass\nclass AccumulateStep(LearnerCallback):\n    \"\"\"\n    Does accumlated step every nth step by accumulating gradients\n    \"\"\"\n    def __init__(self, learn:Learner, n_step:int = 1):\n        super().__init__(learn)\n        self.n_step = n_step\n\n    def on_epoch_begin(self, **kwargs):\n        \"init samples and batches, change optimizer\"\n        self.acc_batches = 0\n        \n    def on_batch_begin(self, last_input, last_target, **kwargs):\n        \"accumulate samples and batches\"\n        self.acc_batches += 1\n    def on_backward_end(self, **kwargs):\n        \"step if number of desired batches accumulated, reset samples\"\n        if (self.acc_batches % self.n_step) == self.n_step - 1:\n            for p in (self.learn.model.parameters()):\n                if p.requires_grad: p.grad.div_(self.acc_batches)\n    \n            self.learn.opt.real_step()\n            self.learn.opt.real_zero_grad()\n            self.acc_batches = 0\n    \n    def on_epoch_end(self, **kwargs):\n        \"step the rest of the accumulated grads\"\n        if self.acc_batches > 0:\n            for p in (self.learn.model.parameters()):\n                if p.requires_grad: p.grad.div_(self.acc_batches)\n            self.learn.opt.real_step()\n            self.learn.opt.real_zero_grad()\n            self.acc_batches = 0\ndef set_BN_momentum(model,momentum=0.1*16/64):\n    for i, (name, layer) in enumerate(model.named_modules()):\n        if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.BatchNorm1d):\n            layer.momentum = momentum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\nimport cv2\nbs = 32\nim_list = ImageList.from_df(df_train, path=DATA/\"stage_1_train_images\")\ntest_fnames = pd.DataFrame(\"ID_\" + pd.read_csv(DATA/\"stage_1_sample_submission.csv\")[\"ID\"].str.split(\"_\", n=2, expand = True)[1].unique() + \".dcm\")\ntest_im_list = ImageList.from_df(test_fnames, path=DATA/\"stage_1_test_images\")\n\ntfms = get_transforms(do_flip=True, \n             flip_vert=False, \n             max_rotate=10)\ninfo=[]\nskf=StratifiedKFold(n_splits=5,shuffle=True,random_state=2019)\nfor train_id,val_id in skf.split(df_train.index,df_train['labels']):\n    info.append([train_id,val_id])\ni=0\nfor train_id,val_id in skf.split(df_train.index,df_train['labels']):\n        data = (im_list.split_by_idxs(info[3][0],info[3][1])\n               .label_from_df(label_delim=\" \")\n               .transform(tfms, size=332)\n               .add_test(test_im_list)\n               .databunch(bs=bs)\n               .normalize(imagenet_stats))\n        f=\"sub\"+str(i)+'.csv'\n        print(f)\n        learn = Learner(data, model,metrics=[accuracy_thresh])\n        n_acc=2\n        learn.callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy_thresh'),AccumulateStep(learn,n_acc)] \n        learn.split( lambda m: (model._conv_head,) )\n        learn.freeze()\n        learn.model_dir=\"/kaggle/working/\"\n        learn.fit_one_cycle(1,2e-3,wd=.0001)\n        learn.save('models')\n        print('done')\n        \"\"\"submission = pd.read_csv(DATA/\"stage_1_sample_submission.csv\")\n        preds = learn.TTA(ds_type=DatasetType.Test)\n        preds = np.array(preds[0])\n        any_probs = 1 - np.prod(1 - preds, axis=1)\n        submission.Label = np.hstack([preds, np.expand_dims(any_probs, -1)]).reshape(-1)\n        submission.head()\n        submission.to_csv(f, index=False)\"\"\"\n        i=i+1\n        print(i)\n        learn.destroy()\n        del learn\n        torch.cuda.empty_cache()\n        #d=int(input())\n        print('enter 1 if you donot want to coninue')\n        \"\"\"one epoch will take 70 minutes in total 6 hours for one fold CAREFULL OF STABLE INTERNEL IF YOU LOOSE CONNECTION\n        THAN YOU ARE F****D WITH 6 HOURS OF GPU\"\"\" \n        #if d==1:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn = Learner(data, model,metrics=[accuracy_thresh])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn.load(\"/tmp/0/bestmodel_0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filehandler = open('fold.pkl',\"wb\")\n#pickle.dump(learn,filehandler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\n\ndef create_download_link(title = \"Download pickle file\", filename=\"fold.pkl\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\ncreate_download_link()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.basic_train import Learner\nfrom fastai.callbacks.one_cycle import OneCycleScheduler\nfrom fastai.core import Floats,Any\n\n\nclass PartialOneCycleScheduler(OneCycleScheduler):\n    def __init__(self, learn:Learner, lr_max:float,                  \n                 moms:Floats=(0.95,0.85), \n                 div_factor:float=25., pct_start:float=0.3,\n                 tot_epochs:int=-1, start_epoch:int=0):\n        super().__init__(learn, lr_max, moms, div_factor, pct_start)\n        self.start_epoch = start_epoch        \n        self.tot_epochs = tot_epochs\n            \n    def on_train_begin(self, n_epochs:int, **kwargs:Any)->None:\n        if self.tot_epochs < 0: self.tot_epochs = n_epochs + self.start_epoch        \n        super().on_train_begin(self.tot_epochs, **kwargs)\n                      \n        self.start_iter = len(self.learn.data.train_dl) * self.start_epoch                \n        for _ in range(self.start_iter):\n            super().on_batch_end(True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}